{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505194ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import agno\n",
    "print(dir(agno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a492067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agent', 'api', 'app', 'cli', 'constants', 'debug', 'document', 'embedder', 'eval', 'exceptions', 'file', 'infra', 'knowledge', 'media', 'memory', 'models', 'playground', 'reasoning', 'reranker', 'run', 'storage', 'team', 'tools', 'utils', 'vectordb', 'workflow', 'workspace']\n"
     ]
    }
   ],
   "source": [
    "import pkgutil\n",
    "import agno\n",
    "\n",
    "submodules = [name for _, name, _ in pkgutil.iter_modules(agno.__path__)]\n",
    "print(submodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55220b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent', 'AgentKnowledge', 'AgentMemory', 'AgentSession', 'Function', 'Memory', 'MemoryUpdateCompletedEvent', 'MemoryUpdateStartedEvent', 'Message', 'ReasoningCompletedEvent', 'ReasoningStartedEvent', 'ReasoningStepEvent', 'RunEvent', 'RunResponse', 'RunResponseCancelledEvent', 'RunResponseCompletedEvent', 'RunResponseContentEvent', 'RunResponseContinuedEvent', 'RunResponseErrorEvent', 'RunResponseEvent', 'RunResponsePausedEvent', 'RunResponseStartedEvent', 'Storage', 'ToolCallCompletedEvent', 'ToolCallStartedEvent', 'Toolkit', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'agent', 'metrics']\n"
     ]
    }
   ],
   "source": [
    "import agno.agent\n",
    "print(dir(agno.agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098963d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AgentMemory', 'Memory', 'MemoryRow', 'TeamMemory', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'agent', 'classifier', 'db', 'manager', 'memory', 'row', 'summarizer', 'summary', 'team', 'v2']\n"
     ]
    }
   ],
   "source": [
    "import agno.memory\n",
    "print(dir(agno.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d000304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base', 'message', 'response']\n",
      "['Function', 'FunctionCall', 'Toolkit', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'decorator', 'function', 'tool', 'toolkit']\n"
     ]
    }
   ],
   "source": [
    "import agno.models\n",
    "print(dir(agno.models))\n",
    "\n",
    "import agno.tools\n",
    "print(dir(agno.tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b4e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': FieldInfo(annotation=str, required=True),\n",
       " 'id': FieldInfo(annotation=Union[str, NoneType], required=False, default=None),\n",
       " 'topic': FieldInfo(annotation=Union[str, NoneType], required=False, default=None),\n",
       " 'input': FieldInfo(annotation=Union[str, NoneType], required=False, default=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agno.memory import Memory\n",
    "Memory.model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4044ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OpenAIChat in module agno.models.openai.chat:\n",
      "\n",
      "class OpenAIChat(agno.models.base.Model)\n",
      " |  OpenAIChat(id: str = 'gpt-4o', name: str = 'OpenAIChat', provider: str = 'OpenAI', supports_native_structured_outputs: bool = True, supports_json_schema_outputs: bool = False, _tool_choice: Union[str, Dict[str, Any], NoneType] = None, system_prompt: Optional[str] = None, instructions: Optional[List[str]] = None, tool_message_role: str = 'tool', assistant_message_role: str = 'assistant', store: Optional[bool] = None, reasoning_effort: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, frequency_penalty: Optional[float] = None, logit_bias: Optional[Any] = None, logprobs: Optional[bool] = None, top_logprobs: Optional[int] = None, max_tokens: Optional[int] = None, max_completion_tokens: Optional[int] = None, modalities: Optional[List[str]] = None, audio: Optional[Dict[str, Any]] = None, presence_penalty: Optional[float] = None, seed: Optional[int] = None, stop: Union[str, List[str], NoneType] = None, temperature: Optional[float] = None, user: Optional[str] = None, top_p: Optional[float] = None, extra_headers: Optional[Any] = None, extra_query: Optional[Any] = None, request_params: Optional[Dict[str, Any]] = None, role_map: Optional[Dict[str, str]] = None, api_key: Optional[str] = None, organization: Optional[str] = None, base_url: Union[str, httpx.URL, NoneType] = None, timeout: Optional[float] = None, max_retries: Optional[int] = None, default_headers: Optional[Any] = None, default_query: Optional[Any] = None, http_client: Optional[httpx.Client] = None, client_params: Optional[Dict[str, Any]] = None) -> None\n",
      " |  \n",
      " |  A class for interacting with OpenAI models using the Chat completions API.\n",
      " |  \n",
      " |  For more information, see: https://platform.openai.com/docs/api-reference/chat/create\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OpenAIChat\n",
      " |      agno.models.base.Model\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, id: str = 'gpt-4o', name: str = 'OpenAIChat', provider: str = 'OpenAI', supports_native_structured_outputs: bool = True, supports_json_schema_outputs: bool = False, _tool_choice: Union[str, Dict[str, Any], NoneType] = None, system_prompt: Optional[str] = None, instructions: Optional[List[str]] = None, tool_message_role: str = 'tool', assistant_message_role: str = 'assistant', store: Optional[bool] = None, reasoning_effort: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, frequency_penalty: Optional[float] = None, logit_bias: Optional[Any] = None, logprobs: Optional[bool] = None, top_logprobs: Optional[int] = None, max_tokens: Optional[int] = None, max_completion_tokens: Optional[int] = None, modalities: Optional[List[str]] = None, audio: Optional[Dict[str, Any]] = None, presence_penalty: Optional[float] = None, seed: Optional[int] = None, stop: Union[str, List[str], NoneType] = None, temperature: Optional[float] = None, user: Optional[str] = None, top_p: Optional[float] = None, extra_headers: Optional[Any] = None, extra_query: Optional[Any] = None, request_params: Optional[Dict[str, Any]] = None, role_map: Optional[Dict[str, str]] = None, api_key: Optional[str] = None, organization: Optional[str] = None, base_url: Union[str, httpx.URL, NoneType] = None, timeout: Optional[float] = None, max_retries: Optional[int] = None, default_headers: Optional[Any] = None, default_query: Optional[Any] = None, http_client: Optional[httpx.Client] = None, client_params: Optional[Dict[str, Any]] = None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  async ainvoke(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> openai.types.chat.chat_completion.ChatCompletion\n",
      " |      Sends an asynchronous chat completion request to the OpenAI API.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages (List[Message]): A list of messages to send to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          ChatCompletion: The chat completion response from the API.\n",
      " |  \n",
      " |  async ainvoke_stream(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> collections.abc.AsyncIterator[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]\n",
      " |      Sends an asynchronous streaming chat completion request to the OpenAI API.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages (List[Message]): A list of messages to send to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Any: An asynchronous iterator of chat completion chunks.\n",
      " |  \n",
      " |  get_async_client(self) -> openai.AsyncOpenAI\n",
      " |      Returns an asynchronous OpenAI client.\n",
      " |      \n",
      " |      Returns:\n",
      " |          AsyncOpenAIClient: An instance of the asynchronous OpenAI client.\n",
      " |  \n",
      " |  get_client(self) -> openai.OpenAI\n",
      " |      Returns an OpenAI client.\n",
      " |      \n",
      " |      Returns:\n",
      " |          OpenAIClient: An instance of the OpenAI client.\n",
      " |  \n",
      " |  get_request_params(self, response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Returns keyword arguments for API requests.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[str, Any]: A dictionary of keyword arguments for API requests.\n",
      " |  \n",
      " |  invoke(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> openai.types.chat.chat_completion.ChatCompletion\n",
      " |      Send a chat completion request to the OpenAI API.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages (List[Message]): A list of messages to send to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          ChatCompletion: The chat completion response from the API.\n",
      " |  \n",
      " |  invoke_stream(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> Iterator[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]\n",
      " |      Send a streaming chat completion request to the OpenAI API.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages (List[Message]): A list of messages to send to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Iterator[ChatCompletionChunk]: An iterator of chat completion chunks.\n",
      " |  \n",
      " |  parse_provider_response(self, response: openai.types.chat.chat_completion.ChatCompletion, response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None) -> agno.models.response.ModelResponse\n",
      " |      Parse the OpenAI response into a ModelResponse.\n",
      " |  \n",
      " |  parse_provider_response_delta(self, response_delta: openai.types.chat.chat_completion_chunk.ChatCompletionChunk) -> agno.models.response.ModelResponse\n",
      " |      Parse the OpenAI streaming response into a ModelResponse.\n",
      " |      \n",
      " |      Args:\n",
      " |          response_delta: Raw response chunk from OpenAI\n",
      " |      \n",
      " |      Returns:\n",
      " |          ModelResponse: Parsed response data\n",
      " |  \n",
      " |  to_dict(self) -> Dict[str, Any]\n",
      " |      Convert the model to a dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[str, Any]: The dictionary representation of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  parse_tool_calls(tool_calls_data: List[openai.types.chat.chat_completion_chunk.ChoiceDeltaToolCall]) -> List[Dict[str, Any]]\n",
      " |      Build tool calls from streamed tool call data.\n",
      " |      \n",
      " |      Args:\n",
      " |          tool_calls_data (List[ChoiceDeltaToolCall]): The tool call data to build from.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Dict[str, Any]]: The built tool calls.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'api_key': typing.Optional[str], 'audio': typing.Op...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'_tool_choice': Field(name='_tool_choice',type...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('id', 'name', 'provider', 'supports_native_structure...\n",
      " |  \n",
      " |  api_key = None\n",
      " |  \n",
      " |  audio = None\n",
      " |  \n",
      " |  base_url = None\n",
      " |  \n",
      " |  client_params = None\n",
      " |  \n",
      " |  default_headers = None\n",
      " |  \n",
      " |  default_query = None\n",
      " |  \n",
      " |  default_role_map = {'assistant': 'assistant', 'model': 'assistant', 's...\n",
      " |  \n",
      " |  extra_headers = None\n",
      " |  \n",
      " |  extra_query = None\n",
      " |  \n",
      " |  frequency_penalty = None\n",
      " |  \n",
      " |  http_client = None\n",
      " |  \n",
      " |  id = 'gpt-4o'\n",
      " |  \n",
      " |  logit_bias = None\n",
      " |  \n",
      " |  logprobs = None\n",
      " |  \n",
      " |  max_completion_tokens = None\n",
      " |  \n",
      " |  max_retries = None\n",
      " |  \n",
      " |  max_tokens = None\n",
      " |  \n",
      " |  metadata = None\n",
      " |  \n",
      " |  modalities = None\n",
      " |  \n",
      " |  name = 'OpenAIChat'\n",
      " |  \n",
      " |  organization = None\n",
      " |  \n",
      " |  presence_penalty = None\n",
      " |  \n",
      " |  provider = 'OpenAI'\n",
      " |  \n",
      " |  reasoning_effort = None\n",
      " |  \n",
      " |  request_params = None\n",
      " |  \n",
      " |  role_map = None\n",
      " |  \n",
      " |  seed = None\n",
      " |  \n",
      " |  stop = None\n",
      " |  \n",
      " |  store = None\n",
      " |  \n",
      " |  supports_native_structured_outputs = True\n",
      " |  \n",
      " |  temperature = None\n",
      " |  \n",
      " |  timeout = None\n",
      " |  \n",
      " |  top_logprobs = None\n",
      " |  \n",
      " |  top_p = None\n",
      " |  \n",
      " |  user = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from agno.models.base.Model:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |      Create a deep copy of the Model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo (dict): Dictionary of objects already copied during the current copying pass.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Model: A new Model instance with deeply copied attributes.\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  async aprocess_response_stream(self, messages: List[agno.models.message.Message], assistant_message: agno.models.message.Message, stream_data: agno.models.base.MessageData, response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> AsyncIterator[agno.models.response.ModelResponse]\n",
      " |      Process a streaming response from the model.\n",
      " |  \n",
      " |  async aresponse(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, functions: Optional[Dict[str, agno.tools.function.Function]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None, tool_call_limit: Optional[int] = None) -> agno.models.response.ModelResponse\n",
      " |      Generate an asynchronous response from the model.\n",
      " |  \n",
      " |  async aresponse_stream(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, functions: Optional[Dict[str, agno.tools.function.Function]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None, tool_call_limit: Optional[int] = None, stream_model_response: bool = True) -> AsyncIterator[Union[agno.models.response.ModelResponse, agno.run.response.RunResponseStartedEvent, agno.run.response.RunResponseContentEvent, agno.run.response.RunResponseCompletedEvent, agno.run.response.RunResponseErrorEvent, agno.run.response.RunResponseCancelledEvent, agno.run.response.RunResponsePausedEvent, agno.run.response.RunResponseContinuedEvent, agno.run.response.ReasoningStartedEvent, agno.run.response.ReasoningStepEvent, agno.run.response.ReasoningCompletedEvent, agno.run.response.MemoryUpdateStartedEvent, agno.run.response.MemoryUpdateCompletedEvent, agno.run.response.ToolCallStartedEvent, agno.run.response.ToolCallCompletedEvent, agno.run.response.ParserModelResponseStartedEvent, agno.run.response.ParserModelResponseCompletedEvent, agno.run.team.RunResponseStartedEvent, agno.run.team.RunResponseContentEvent, agno.run.team.RunResponseCompletedEvent, agno.run.team.RunResponseErrorEvent, agno.run.team.RunResponseCancelledEvent, agno.run.team.ReasoningStartedEvent, agno.run.team.ReasoningStepEvent, agno.run.team.ReasoningCompletedEvent, agno.run.team.MemoryUpdateStartedEvent, agno.run.team.MemoryUpdateCompletedEvent, agno.run.team.ToolCallStartedEvent, agno.run.team.ToolCallCompletedEvent, agno.run.team.ParserModelResponseStartedEvent, agno.run.team.ParserModelResponseCompletedEvent]]\n",
      " |      Generate an asynchronous streaming response from the model.\n",
      " |  \n",
      " |  async arun_function_call(self, function_call: agno.tools.function.FunctionCall) -> Tuple[Union[bool, agno.exceptions.AgentRunException], agno.utils.timer.Timer, agno.tools.function.FunctionCall]\n",
      " |      Run a single function call and return its success status, timer, and the FunctionCall object.\n",
      " |  \n",
      " |  async arun_function_calls(self, function_calls: List[agno.tools.function.FunctionCall], function_call_results: List[agno.models.message.Message], additional_messages: Optional[List[agno.models.message.Message]] = None, current_function_call_count: int = 0, function_call_limit: Optional[int] = None, skip_pause_check: bool = False) -> AsyncIterator[Union[agno.models.response.ModelResponse, agno.run.response.RunResponseStartedEvent, agno.run.response.RunResponseContentEvent, agno.run.response.RunResponseCompletedEvent, agno.run.response.RunResponseErrorEvent, agno.run.response.RunResponseCancelledEvent, agno.run.response.RunResponsePausedEvent, agno.run.response.RunResponseContinuedEvent, agno.run.response.ReasoningStartedEvent, agno.run.response.ReasoningStepEvent, agno.run.response.ReasoningCompletedEvent, agno.run.response.MemoryUpdateStartedEvent, agno.run.response.MemoryUpdateCompletedEvent, agno.run.response.ToolCallStartedEvent, agno.run.response.ToolCallCompletedEvent, agno.run.response.ParserModelResponseStartedEvent, agno.run.response.ParserModelResponseCompletedEvent, agno.run.team.RunResponseStartedEvent, agno.run.team.RunResponseContentEvent, agno.run.team.RunResponseCompletedEvent, agno.run.team.RunResponseErrorEvent, agno.run.team.RunResponseCancelledEvent, agno.run.team.ReasoningStartedEvent, agno.run.team.ReasoningStepEvent, agno.run.team.ReasoningCompletedEvent, agno.run.team.MemoryUpdateStartedEvent, agno.run.team.MemoryUpdateCompletedEvent, agno.run.team.ToolCallStartedEvent, agno.run.team.ToolCallCompletedEvent, agno.run.team.ParserModelResponseStartedEvent, agno.run.team.ParserModelResponseCompletedEvent]]\n",
      " |  \n",
      " |  create_function_call_result(self, function_call: agno.tools.function.FunctionCall, success: bool, output: Union[List[Any], str, NoneType] = None, timer: Optional[agno.utils.timer.Timer] = None) -> agno.models.message.Message\n",
      " |      Create a function call result message.\n",
      " |  \n",
      " |  create_tool_call_limit_error_result(self, function_call: agno.tools.function.FunctionCall) -> agno.models.message.Message\n",
      " |  \n",
      " |  format_function_call_results(self, messages: List[agno.models.message.Message], function_call_results: List[agno.models.message.Message], **kwargs) -> None\n",
      " |      Format function call results.\n",
      " |  \n",
      " |  get_function_call_to_run_from_tool_execution(self, tool_execution: agno.models.response.ToolExecution, functions: Optional[Dict[str, agno.tools.function.Function]] = None) -> agno.tools.function.FunctionCall\n",
      " |  \n",
      " |  get_function_calls_to_run(self, assistant_message: agno.models.message.Message, messages: List[agno.models.message.Message], functions: Optional[Dict[str, agno.tools.function.Function]] = None) -> List[agno.tools.function.FunctionCall]\n",
      " |      Prepare function calls for the assistant message.\n",
      " |  \n",
      " |  get_instructions_for_model(self, tools: Optional[List[Any]] = None) -> Optional[List[str]]\n",
      " |  \n",
      " |  get_provider(self) -> str\n",
      " |  \n",
      " |  get_system_message_for_model(self, tools: Optional[List[Any]] = None) -> Optional[str]\n",
      " |  \n",
      " |  process_response_stream(self, messages: List[agno.models.message.Message], assistant_message: agno.models.message.Message, stream_data: agno.models.base.MessageData, response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None) -> Iterator[agno.models.response.ModelResponse]\n",
      " |      Process a streaming response from the model.\n",
      " |  \n",
      " |  response(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, functions: Optional[Dict[str, agno.tools.function.Function]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None, tool_call_limit: Optional[int] = None) -> agno.models.response.ModelResponse\n",
      " |      Generate a response from the model.\n",
      " |  \n",
      " |  response_stream(self, messages: List[agno.models.message.Message], response_format: Union[Dict, Type[pydantic.main.BaseModel], NoneType] = None, tools: Optional[List[Dict[str, Any]]] = None, functions: Optional[Dict[str, agno.tools.function.Function]] = None, tool_choice: Union[str, Dict[str, Any], NoneType] = None, tool_call_limit: Optional[int] = None, stream_model_response: bool = True) -> Iterator[Union[agno.models.response.ModelResponse, agno.run.response.RunResponseStartedEvent, agno.run.response.RunResponseContentEvent, agno.run.response.RunResponseCompletedEvent, agno.run.response.RunResponseErrorEvent, agno.run.response.RunResponseCancelledEvent, agno.run.response.RunResponsePausedEvent, agno.run.response.RunResponseContinuedEvent, agno.run.response.ReasoningStartedEvent, agno.run.response.ReasoningStepEvent, agno.run.response.ReasoningCompletedEvent, agno.run.response.MemoryUpdateStartedEvent, agno.run.response.MemoryUpdateCompletedEvent, agno.run.response.ToolCallStartedEvent, agno.run.response.ToolCallCompletedEvent, agno.run.response.ParserModelResponseStartedEvent, agno.run.response.ParserModelResponseCompletedEvent, agno.run.team.RunResponseStartedEvent, agno.run.team.RunResponseContentEvent, agno.run.team.RunResponseCompletedEvent, agno.run.team.RunResponseErrorEvent, agno.run.team.RunResponseCancelledEvent, agno.run.team.ReasoningStartedEvent, agno.run.team.ReasoningStepEvent, agno.run.team.ReasoningCompletedEvent, agno.run.team.MemoryUpdateStartedEvent, agno.run.team.MemoryUpdateCompletedEvent, agno.run.team.ToolCallStartedEvent, agno.run.team.ToolCallCompletedEvent, agno.run.team.ParserModelResponseStartedEvent, agno.run.team.ParserModelResponseCompletedEvent]]\n",
      " |      Generate a streaming response from the model.\n",
      " |  \n",
      " |  run_function_call(self, function_call: agno.tools.function.FunctionCall, function_call_results: List[agno.models.message.Message], additional_messages: Optional[List[agno.models.message.Message]] = None) -> Iterator[Union[agno.models.response.ModelResponse, agno.run.response.RunResponseStartedEvent, agno.run.response.RunResponseContentEvent, agno.run.response.RunResponseCompletedEvent, agno.run.response.RunResponseErrorEvent, agno.run.response.RunResponseCancelledEvent, agno.run.response.RunResponsePausedEvent, agno.run.response.RunResponseContinuedEvent, agno.run.response.ReasoningStartedEvent, agno.run.response.ReasoningStepEvent, agno.run.response.ReasoningCompletedEvent, agno.run.response.MemoryUpdateStartedEvent, agno.run.response.MemoryUpdateCompletedEvent, agno.run.response.ToolCallStartedEvent, agno.run.response.ToolCallCompletedEvent, agno.run.response.ParserModelResponseStartedEvent, agno.run.response.ParserModelResponseCompletedEvent, agno.run.team.RunResponseStartedEvent, agno.run.team.RunResponseContentEvent, agno.run.team.RunResponseCompletedEvent, agno.run.team.RunResponseErrorEvent, agno.run.team.RunResponseCancelledEvent, agno.run.team.ReasoningStartedEvent, agno.run.team.ReasoningStepEvent, agno.run.team.ReasoningCompletedEvent, agno.run.team.MemoryUpdateStartedEvent, agno.run.team.MemoryUpdateCompletedEvent, agno.run.team.ToolCallStartedEvent, agno.run.team.ToolCallCompletedEvent, agno.run.team.ParserModelResponseStartedEvent, agno.run.team.ParserModelResponseCompletedEvent]]\n",
      " |  \n",
      " |  run_function_calls(self, function_calls: List[agno.tools.function.FunctionCall], function_call_results: List[agno.models.message.Message], additional_messages: Optional[List[agno.models.message.Message]] = None, current_function_call_count: int = 0, function_call_limit: Optional[int] = None) -> Iterator[Union[agno.models.response.ModelResponse, agno.run.response.RunResponseStartedEvent, agno.run.response.RunResponseContentEvent, agno.run.response.RunResponseCompletedEvent, agno.run.response.RunResponseErrorEvent, agno.run.response.RunResponseCancelledEvent, agno.run.response.RunResponsePausedEvent, agno.run.response.RunResponseContinuedEvent, agno.run.response.ReasoningStartedEvent, agno.run.response.ReasoningStepEvent, agno.run.response.ReasoningCompletedEvent, agno.run.response.MemoryUpdateStartedEvent, agno.run.response.MemoryUpdateCompletedEvent, agno.run.response.ToolCallStartedEvent, agno.run.response.ToolCallCompletedEvent, agno.run.response.ParserModelResponseStartedEvent, agno.run.response.ParserModelResponseCompletedEvent, agno.run.team.RunResponseStartedEvent, agno.run.team.RunResponseContentEvent, agno.run.team.RunResponseCompletedEvent, agno.run.team.RunResponseErrorEvent, agno.run.team.RunResponseCancelledEvent, agno.run.team.ReasoningStartedEvent, agno.run.team.ReasoningStepEvent, agno.run.team.ReasoningCompletedEvent, agno.run.team.MemoryUpdateStartedEvent, agno.run.team.MemoryUpdateCompletedEvent, agno.run.team.ToolCallStartedEvent, agno.run.team.ToolCallCompletedEvent, agno.run.team.ParserModelResponseStartedEvent, agno.run.team.ParserModelResponseCompletedEvent]]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from agno.models.base.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from agno.models.base.Model:\n",
      " |  \n",
      " |  assistant_message_role = 'assistant'\n",
      " |  \n",
      " |  instructions = None\n",
      " |  \n",
      " |  supports_json_schema_outputs = False\n",
      " |  \n",
      " |  system_prompt = None\n",
      " |  \n",
      " |  tool_message_role = 'tool'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agno.models.openai.chat import OpenAIChat\n",
    "help(OpenAIChat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valura_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
